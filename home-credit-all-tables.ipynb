{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description\nThis notebook merges all tables with some simple feature engineering and runs our models on the aggregated dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:30.609676Z","iopub.execute_input":"2022-04-25T14:19:30.610223Z","iopub.status.idle":"2022-04-25T14:19:30.637666Z","shell.execute_reply.started":"2022-04-25T14:19:30.610137Z","shell.execute_reply":"2022-04-25T14:19:30.637071Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../input/\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:33.278366Z","iopub.execute_input":"2022-04-25T14:19:33.278633Z","iopub.status.idle":"2022-04-25T14:19:33.284726Z","shell.execute_reply.started":"2022-04-25T14:19:33.278603Z","shell.execute_reply":"2022-04-25T14:19:33.283634Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:35.219225Z","iopub.execute_input":"2022-04-25T14:19:35.219480Z","iopub.status.idle":"2022-04-25T14:19:35.233091Z","shell.execute_reply.started":"2022-04-25T14:19:35.219452Z","shell.execute_reply":"2022-04-25T14:19:35.231581Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('category'))\n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:35.904694Z","iopub.execute_input":"2022-04-25T14:19:35.904985Z","iopub.status.idle":"2022-04-25T14:19:35.915852Z","shell.execute_reply.started":"2022-04-25T14:19:35.904953Z","shell.execute_reply":"2022-04-25T14:19:35.914610Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import sys\n\ndef return_size(df):\n    \"\"\"Return size of dataframe in gigabytes\"\"\"\n    return round(sys.getsizeof(df) / 1e9, 2)\n\ndef convert_types(df, print_info = False):\n    \n    original_memory = df.memory_usage().sum()\n    \n    # Iterate through each column\n    for c in df:\n        \n        # Convert ids and booleans to integers\n        if ('SK_ID' in c):\n            df[c] = df[c].fillna(0).astype(np.int32)\n            \n        # Convert objects to category\n        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n            df[c] = df[c].astype('category')\n        \n        # Booleans mapped to integers\n        elif list(df[c].unique()) == [1, 0]:\n            df[c] = df[c].astype(bool)\n        \n        # Float64 to float32\n        elif df[c].dtype == float:\n            df[c] = df[c].astype(np.float32)\n            \n        # Int64 to int32\n        elif df[c].dtype == int:\n            df[c] = df[c].astype(np.int32)\n        \n    new_memory = df.memory_usage().sum()\n    \n    if print_info:\n        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:36.541218Z","iopub.execute_input":"2022-04-25T14:19:36.541567Z","iopub.status.idle":"2022-04-25T14:19:36.553233Z","shell.execute_reply.started":"2022-04-25T14:19:36.541538Z","shell.execute_reply":"2022-04-25T14:19:36.552570Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def aggregate_client(df, group_vars, df_names):\n    \"\"\"Aggregate a dataframe with data at the loan level \n    at the client level\n    \n    Args:\n        df (dataframe): data at the loan level\n        group_vars (list of two strings): grouping variables for the loan \n        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n        names (list of two strings): names to call the resulting columns\n        (example ['cash', 'client'])\n        \n    Returns:\n        df_client (dataframe): aggregated numeric stats at the client level. \n        Each client will have a single row with all the numeric data aggregated\n    \"\"\"\n    \n    # Aggregate the numeric columns\n    df_agg = agg_numeric(df, parent_var=group_vars[0], df_name=df_names[0])\n    \n    # If there are categorical variables\n    if any(df.dtypes == 'category'):\n    \n        # Count the categorical columns\n        df_counts = agg_categorical(df, parent_var=group_vars[0], df_name=df_names[0])\n\n        # Merge the numeric and categorical\n        df_by_loan = df_counts.merge(df_agg, on=group_vars[0], how='outer')\n\n        gc.enable()\n        del df_agg, df_counts\n        gc.collect()\n\n        # Merge to get the client id in dataframe\n        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on=group_vars[0], how='left')\n\n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns=[group_vars[0]])\n\n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var=group_vars[1], df_name=df_names[1])\n\n        \n    # No categorical variables\n    else:\n        # Merge to get the client id in dataframe\n        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on=group_vars[0], how='left')\n        \n        gc.enable()\n        del df_agg\n        gc.collect()\n        \n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns=[group_vars[0]])\n        \n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var=group_vars[1], df_name=df_names[1])\n        \n    # Memory management\n    gc.enable()\n    del df, df_by_loan\n    gc.collect()\n\n    return df_by_client","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:37.099384Z","iopub.execute_input":"2022-04-25T14:19:37.099647Z","iopub.status.idle":"2022-04-25T14:19:37.114434Z","shell.execute_reply.started":"2022-04-25T14:19:37.099618Z","shell.execute_reply":"2022-04-25T14:19:37.113580Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"bureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\nbureau = convert_types(bureau, print_info=True)\nbureau.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:37.992187Z","iopub.execute_input":"2022-04-25T14:19:37.992433Z","iopub.status.idle":"2022-04-25T14:19:45.070892Z","shell.execute_reply.started":"2022-04-25T14:19:37.992406Z","shell.execute_reply":"2022-04-25T14:19:45.070165Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"bureau_agg = agg_numeric(bureau.drop(columns=['SK_ID_BUREAU']), 'SK_ID_CURR', 'bureau')\nbureau_agg.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:45.072290Z","iopub.execute_input":"2022-04-25T14:19:45.072502Z","iopub.status.idle":"2022-04-25T14:19:51.028010Z","shell.execute_reply.started":"2022-04-25T14:19:45.072477Z","shell.execute_reply":"2022-04-25T14:19:51.026751Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"bureau_counts = agg_categorical(bureau, 'SK_ID_CURR', 'bureau')\nbureau_counts.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:51.029328Z","iopub.execute_input":"2022-04-25T14:19:51.029569Z","iopub.status.idle":"2022-04-25T14:19:59.383769Z","shell.execute_reply.started":"2022-04-25T14:19:51.029538Z","shell.execute_reply":"2022-04-25T14:19:59.382453Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"bureau_balance = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv')\nbureau_balance = convert_types(bureau_balance, print_info=True)\nbureau_balance.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:19:59.387868Z","iopub.execute_input":"2022-04-25T14:19:59.389261Z","iopub.status.idle":"2022-04-25T14:20:19.034898Z","shell.execute_reply.started":"2022-04-25T14:19:59.389183Z","shell.execute_reply":"2022-04-25T14:20:19.033580Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"bureau_balance_counts = agg_categorical(bureau_balance, 'SK_ID_BUREAU', 'bureau_balance')\nbureau_balance_counts.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:20:19.037004Z","iopub.execute_input":"2022-04-25T14:20:19.037434Z","iopub.status.idle":"2022-04-25T14:20:36.614322Z","shell.execute_reply.started":"2022-04-25T14:20:19.037403Z","shell.execute_reply":"2022-04-25T14:20:36.613146Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"bureau_balance_agg = agg_numeric(bureau_balance, 'SK_ID_BUREAU', 'bureau_balance')\nbureau_balance_agg.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:20:36.615841Z","iopub.execute_input":"2022-04-25T14:20:36.616578Z","iopub.status.idle":"2022-04-25T14:20:43.438309Z","shell.execute_reply.started":"2022-04-25T14:20:36.616508Z","shell.execute_reply":"2022-04-25T14:20:43.437474Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index=True, left_on='SK_ID_BUREAU', how='outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on='SK_ID_BUREAU', how='left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns=['SK_ID_BUREAU']), 'SK_ID_CURR', 'client')\n\nbureau_balance_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:20:43.439572Z","iopub.execute_input":"2022-04-25T14:20:43.443360Z","iopub.status.idle":"2022-04-25T14:20:55.701602Z","shell.execute_reply.started":"2022-04-25T14:20:43.443269Z","shell.execute_reply":"2022-04-25T14:20:55.700106Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Load training data\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\napp_test = convert_types(app_train, print_info=True)\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:20:55.703078Z","iopub.execute_input":"2022-04-25T14:20:55.703317Z","iopub.status.idle":"2022-04-25T14:21:04.820283Z","shell.execute_reply.started":"2022-04-25T14:20:55.703284Z","shell.execute_reply":"2022-04-25T14:21:04.819109Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load testing data\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\napp_test = convert_types(app_test, print_info=True)\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:04.821746Z","iopub.execute_input":"2022-04-25T14:21:04.821976Z","iopub.status.idle":"2022-04-25T14:21:06.353827Z","shell.execute_reply.started":"2022-04-25T14:21:04.821947Z","shell.execute_reply":"2022-04-25T14:21:06.352327Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'category':\n        # For binary columns, encode with 0 and 1 (indeed the same as one-hot encoding)\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:06.358327Z","iopub.execute_input":"2022-04-25T14:21:06.359325Z","iopub.status.idle":"2022-04-25T14:21:07.949565Z","shell.execute_reply.started":"2022-04-25T14:21:06.359235Z","shell.execute_reply":"2022-04-25T14:21:07.948035Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\n# The resulting tables (ignore the target column) have different number of columns\n# Because some values occur only in the training data\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:07.950828Z","iopub.execute_input":"2022-04-25T14:21:07.951114Z","iopub.status.idle":"2022-04-25T14:21:08.770702Z","shell.execute_reply.started":"2022-04-25T14:21:07.951077Z","shell.execute_reply":"2022-04-25T14:21:08.769152Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Merge with the value counts of bureau\ntrain = app_train.merge(bureau_counts, on='SK_ID_CURR', how='left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on='SK_ID_CURR', how='left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on='SK_ID_CURR', how='left')\n\nprint('Training data shape: ', train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:08.772982Z","iopub.execute_input":"2022-04-25T14:21:08.774247Z","iopub.status.idle":"2022-04-25T14:21:13.889741Z","shell.execute_reply.started":"2022-04-25T14:21:08.774177Z","shell.execute_reply":"2022-04-25T14:21:13.889022Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Merge with the value counts of bureau\ntest = app_test.merge(bureau_counts, on='SK_ID_CURR', how='left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on='SK_ID_CURR', how='left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on='SK_ID_CURR', how='left')\n\nprint('Testing data shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:13.890751Z","iopub.execute_input":"2022-04-25T14:21:13.891010Z","iopub.status.idle":"2022-04-25T14:21:14.906652Z","shell.execute_reply.started":"2022-04-25T14:21:13.890972Z","shell.execute_reply":"2022-04-25T14:21:14.905455Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"gc.enable()\ndel bureau, bureau_counts, bureau_agg, bureau_balance, bureau_by_loan, bureau_balance_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:14.908276Z","iopub.execute_input":"2022-04-25T14:21:14.908575Z","iopub.status.idle":"2022-04-25T14:21:15.092602Z","shell.execute_reply.started":"2022-04-25T14:21:14.908534Z","shell.execute_reply":"2022-04-25T14:21:15.091396Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"previous = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\nprevious = convert_types(previous, print_info=True)\nprevious.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:15.093949Z","iopub.execute_input":"2022-04-25T14:21:15.094203Z","iopub.status.idle":"2022-04-25T14:21:37.358757Z","shell.execute_reply.started":"2022-04-25T14:21:15.094171Z","shell.execute_reply":"2022-04-25T14:21:37.357757Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Calculate aggregate statistics for each numeric column\nprevious_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\nprint('Previous aggregation shape: ', previous_agg.shape)\nprevious_agg.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:37.359928Z","iopub.execute_input":"2022-04-25T14:21:37.360153Z","iopub.status.idle":"2022-04-25T14:21:47.055326Z","shell.execute_reply.started":"2022-04-25T14:21:37.360125Z","shell.execute_reply":"2022-04-25T14:21:47.054442Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Calculate value counts for each categorical column\nprevious_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\nprint('Previous counts shape: ', previous_counts.shape)\nprevious_counts.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:21:47.056484Z","iopub.execute_input":"2022-04-25T14:21:47.057554Z","iopub.status.idle":"2022-04-25T14:22:38.348147Z","shell.execute_reply.started":"2022-04-25T14:21:47.057483Z","shell.execute_reply":"2022-04-25T14:22:38.347410Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Merge in the previous information\ntrain = train.merge(previous_counts, on='SK_ID_CURR', how='left')\ntrain = train.merge(previous_agg, on='SK_ID_CURR', how='left')\n\ntest = test.merge(previous_counts, on='SK_ID_CURR', how='left')\ntest = test.merge(previous_agg, on='SK_ID_CURR', how='left')\n\n# Remove variables to free memory\ngc.enable()\ndel previous, previous_agg, previous_counts\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:22:38.349211Z","iopub.execute_input":"2022-04-25T14:22:38.350154Z","iopub.status.idle":"2022-04-25T14:23:19.646740Z","shell.execute_reply.started":"2022-04-25T14:22:38.350075Z","shell.execute_reply":"2022-04-25T14:23:19.646028Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"cash = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\ncash = convert_types(cash, print_info=True)\ncash.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:23:19.647946Z","iopub.execute_input":"2022-04-25T14:23:19.649019Z","iopub.status.idle":"2022-04-25T14:23:34.928757Z","shell.execute_reply.started":"2022-04-25T14:23:19.648975Z","shell.execute_reply":"2022-04-25T14:23:34.927483Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"cash_by_client = aggregate_client(cash, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['cash', 'client'])\ncash_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:23:34.930733Z","iopub.execute_input":"2022-04-25T14:23:34.931188Z","iopub.status.idle":"2022-04-25T14:25:15.432920Z","shell.execute_reply.started":"2022-04-25T14:23:34.931158Z","shell.execute_reply":"2022-04-25T14:25:15.431795Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print('Cash by Client Shape: ', cash_by_client.shape)\n\ntrain = train.merge(cash_by_client, on='SK_ID_CURR', how='left')\ntest = test.merge(cash_by_client, on='SK_ID_CURR', how='left')\n\ngc.enable()\ndel cash, cash_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:25:15.434999Z","iopub.execute_input":"2022-04-25T14:25:15.435327Z","iopub.status.idle":"2022-04-25T14:25:23.481289Z","shell.execute_reply.started":"2022-04-25T14:25:15.435295Z","shell.execute_reply":"2022-04-25T14:25:23.479634Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"credit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\ncredit = convert_types(credit, print_info=True)\ncredit.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:25:23.482512Z","iopub.execute_input":"2022-04-25T14:25:23.482806Z","iopub.status.idle":"2022-04-25T14:25:41.022955Z","shell.execute_reply.started":"2022-04-25T14:25:23.482767Z","shell.execute_reply":"2022-04-25T14:25:41.021998Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"credit_by_client = aggregate_client(credit, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['credit', 'client'])\ncredit_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:25:41.024249Z","iopub.execute_input":"2022-04-25T14:25:41.024456Z","iopub.status.idle":"2022-04-25T14:26:22.170385Z","shell.execute_reply.started":"2022-04-25T14:25:41.024428Z","shell.execute_reply":"2022-04-25T14:26:22.169736Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print('Credit by client shape: ', credit_by_client.shape)\n\ntrain = train.merge(credit_by_client, on='SK_ID_CURR', how='left')\ntest = test.merge(credit_by_client, on='SK_ID_CURR', how='left')\n\ngc.enable()\ndel credit, credit_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:26:22.171571Z","iopub.execute_input":"2022-04-25T14:26:22.172512Z","iopub.status.idle":"2022-04-25T14:26:40.669807Z","shell.execute_reply.started":"2022-04-25T14:26:22.172447Z","shell.execute_reply":"2022-04-25T14:26:40.668229Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"installments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\ninstallments = convert_types(installments, print_info=True)\ninstallments.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:26:40.671741Z","iopub.execute_input":"2022-04-25T14:26:40.671950Z","iopub.status.idle":"2022-04-25T14:27:05.445324Z","shell.execute_reply.started":"2022-04-25T14:26:40.671917Z","shell.execute_reply":"2022-04-25T14:27:05.444064Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\ninstallments_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:27:05.447246Z","iopub.execute_input":"2022-04-25T14:27:05.447507Z","iopub.status.idle":"2022-04-25T14:28:19.604425Z","shell.execute_reply.started":"2022-04-25T14:27:05.447463Z","shell.execute_reply":"2022-04-25T14:28:19.603840Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print('Installments by client shape: ', installments_by_client.shape)\n\ntrain = train.merge(installments_by_client, on='SK_ID_CURR', how='left')\ntest = test.merge(installments_by_client, on='SK_ID_CURR', how='left')\n\ngc.enable()\ndel installments, installments_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:28:19.605408Z","iopub.execute_input":"2022-04-25T14:28:19.606098Z","iopub.status.idle":"2022-04-25T14:28:24.524431Z","shell.execute_reply.started":"2022-04-25T14:28:19.606060Z","shell.execute_reply":"2022-04-25T14:28:24.523119Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def remove_missing_columns(train, test, threshold = 70):\n    # Calculate missing stats for train and test (remember to calculate a percent!)\n    train_miss = pd.DataFrame(train.isnull().sum())\n    train_miss['percent'] = 100 * train_miss[0] / len(train)\n    \n    test_miss = pd.DataFrame(test.isnull().sum())\n    test_miss['percent'] = 100 * test_miss[0] / len(test)\n    \n    # list of missing columns for train and test\n    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n    \n    # Combine the two lists together\n    missing_columns = list(set(missing_train_columns + missing_test_columns))\n    \n    # Print information\n    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n    \n    # Drop the missing columns and return\n    train = train.drop(columns = missing_columns)\n    test = test.drop(columns = missing_columns)\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:28:24.528451Z","iopub.execute_input":"2022-04-25T14:28:24.528745Z","iopub.status.idle":"2022-04-25T14:28:24.539619Z","shell.execute_reply.started":"2022-04-25T14:28:24.528715Z","shell.execute_reply":"2022-04-25T14:28:24.538746Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train, test = remove_missing_columns(train, test)\nprint('Final Training Shape: ', train.shape)\nprint('Final Testing Shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:28:24.540727Z","iopub.execute_input":"2022-04-25T14:28:24.540962Z","iopub.status.idle":"2022-04-25T14:28:27.467631Z","shell.execute_reply.started":"2022-04-25T14:28:24.540934Z","shell.execute_reply":"2022-04-25T14:28:27.466262Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_labels = train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\ntrain['TARGET'] = train_labels\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:28:27.469192Z","iopub.execute_input":"2022-04-25T14:28:27.469486Z","iopub.status.idle":"2022-04-25T14:28:27.849261Z","shell.execute_reply.started":"2022-04-25T14:28:27.469446Z","shell.execute_reply":"2022-04-25T14:28:27.848361Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train = train.drop(columns=['SK_ID_CURR'])\nsubmit = test[['SK_ID_CURR']]\ntest = test.drop(columns=['SK_ID_CURR'])","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:28:27.851689Z","iopub.execute_input":"2022-04-25T14:28:27.852002Z","iopub.status.idle":"2022-04-25T14:28:28.199706Z","shell.execute_reply.started":"2022-04-25T14:28:27.851961Z","shell.execute_reply":"2022-04-25T14:28:28.198862Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n# Drop the target from the training data\nif 'TARGET' in train:\n    train = train.drop(columns = ['TARGET'])\n    \n# Feature names\nfeatures = list(train.columns)\n\nfor feat in features:\n    # Median imputation of missing values\n    imputer = SimpleImputer(strategy = 'median')\n\n    # Scale each feature to 0-1\n    scaler = MinMaxScaler(feature_range = (0, 1))\n\n    # Fit on the training data\n    imputer.fit(train[feat].values.reshape(-1, 1))\n\n    # Transform both training and testing data\n    train[feat] = imputer.transform(train[feat].values.reshape(-1, 1))\n    test[feat] = imputer.transform(test[feat].values.reshape(-1, 1))\n\n    # Repeat with the scaler\n    scaler.fit(train[feat].values.reshape(-1, 1))\n    train[feat] = scaler.transform(train[feat].values.reshape(-1, 1))\n    test[feat] = scaler.transform(test[feat].values.reshape(-1, 1))\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)\n\ngc.enable()\ndel imputer, scaler\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:28:28.202172Z","iopub.execute_input":"2022-04-25T14:28:28.202856Z","iopub.status.idle":"2022-04-25T14:28:59.470872Z","shell.execute_reply.started":"2022-04-25T14:28:28.202815Z","shell.execute_reply":"2022-04-25T14:28:59.469998Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 1e-4)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:08:43.228392Z","iopub.execute_input":"2022-04-25T14:08:43.228754Z","iopub.status.idle":"2022-04-25T14:09:01.944682Z","shell.execute_reply.started":"2022-04-25T14:08:43.228695Z","shell.execute_reply":"2022-04-25T14:09:01.943786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_reg_pred = log_reg.predict_proba(test)[:, 1]\n\ngc.enable()\ndel log_reg\ngc.collect()\n\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:09:01.946408Z","iopub.execute_input":"2022-04-25T14:09:01.947006Z","iopub.status.idle":"2022-04-25T14:09:02.300737Z","shell.execute_reply.started":"2022-04-25T14:09:01.946948Z","shell.execute_reply":"2022-04-25T14:09:02.299875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_all_table.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:09:02.302242Z","iopub.execute_input":"2022-04-25T14:09:02.302864Z","iopub.status.idle":"2022-04-25T14:09:02.518537Z","shell.execute_reply.started":"2022-04-25T14:09:02.302817Z","shell.execute_reply":"2022-04-25T14:09:02.517532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Private Score: 0.71032, Public Score: 0.70882","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Create a random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 233, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Make predictions\nrf_pred = random_forest.predict_proba(test)[:, 1]\n\ngc.enable()\ndel random_forest\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:09:02.519978Z","iopub.execute_input":"2022-04-25T14:09:02.520321Z","iopub.status.idle":"2022-04-25T14:14:13.276705Z","shell.execute_reply.started":"2022-04-25T14:09:02.520275Z","shell.execute_reply":"2022-04-25T14:14:13.275816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit['TARGET'] = rf_pred\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:14:13.278052Z","iopub.execute_input":"2022-04-25T14:14:13.278295Z","iopub.status.idle":"2022-04-25T14:14:13.290569Z","shell.execute_reply.started":"2022-04-25T14:14:13.278266Z","shell.execute_reply":"2022-04-25T14:14:13.289914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('rf_all_table.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:14:13.292136Z","iopub.execute_input":"2022-04-25T14:14:13.292972Z","iopub.status.idle":"2022-04-25T14:14:13.412141Z","shell.execute_reply.started":"2022-04-25T14:14:13.292922Z","shell.execute_reply":"2022-04-25T14:14:13.411419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Private Score: 0.69622, Public Score: 0.69016","metadata":{}},{"cell_type":"code","source":"# K-fold cross validation\nfrom sklearn.model_selection import KFold\nfolds = KFold(n_splits=10, shuffle=True, random_state=233)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:30:28.116351Z","iopub.execute_input":"2022-04-25T14:30:28.116634Z","iopub.status.idle":"2022-04-25T14:30:28.121386Z","shell.execute_reply.started":"2022-04-25T14:30:28.116603Z","shell.execute_reply":"2022-04-25T14:30:28.120670Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import roc_auc_score\n\noof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    trn_x, trn_y = train.iloc[trn_idx], train_labels.iloc[trn_idx]\n    val_x, val_y = train.iloc[val_idx], train_labels.iloc[val_idx]\n    clf = Ridge(alpha=20, copy_X=True, fit_intercept=True, solver='auto', max_iter=10000, normalize=False, random_state=0, tol=0.0025)\n    clf.fit(trn_x, trn_y)\n    \n    oof_preds[val_idx] = clf.predict(val_x)\n    sub_preds += clf.predict(test) / folds.n_splits\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    del clf, trn_x, trn_y, val_x, val_y\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:14:13.42471Z","iopub.execute_input":"2022-04-25T14:14:13.425187Z","iopub.status.idle":"2022-04-25T14:15:51.10227Z","shell.execute_reply.started":"2022-04-25T14:14:13.425144Z","shell.execute_reply":"2022-04-25T14:15:51.10162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit['TARGET'] = sub_preds\nsubmit.loc[submit['TARGET'] < 0, 'TARGET'] = 0\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:15:51.103441Z","iopub.execute_input":"2022-04-25T14:15:51.10411Z","iopub.status.idle":"2022-04-25T14:15:51.116081Z","shell.execute_reply.started":"2022-04-25T14:15:51.104077Z","shell.execute_reply":"2022-04-25T14:15:51.115442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('ridge_all_table.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:15:51.11775Z","iopub.execute_input":"2022-04-25T14:15:51.11856Z","iopub.status.idle":"2022-04-25T14:15:51.285421Z","shell.execute_reply.started":"2022-04-25T14:15:51.118512Z","shell.execute_reply":"2022-04-25T14:15:51.284784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Private Score: 0.76100, Public Score: 0.76777","metadata":{}},{"cell_type":"code","source":"# Build Lightgbm\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:29:15.262901Z","iopub.execute_input":"2022-04-25T14:29:15.263262Z","iopub.status.idle":"2022-04-25T14:29:16.475153Z","shell.execute_reply.started":"2022-04-25T14:29:15.263227Z","shell.execute_reply":"2022-04-25T14:29:16.473556Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import re\ntrain = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ntest = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nfrom sklearn.metrics import roc_auc_score\n\noof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    trn_x, trn_y = train.iloc[trn_idx], train_labels.iloc[trn_idx]\n    val_x, val_y = train.iloc[val_idx], train_labels.iloc[val_idx]\n    \n    # Create Lightgbm model\n    model = lgb.LGBMClassifier(n_estimators=10000, objective='binary', \n                               class_weight='balanced', learning_rate=0.05, \n                               reg_alpha=0.1, reg_lambda=0.1, \n                               subsample=0.8, n_jobs=-1, random_state=233)\n    \n    model.fit(trn_x, trn_y, eval_metric='auc', eval_set=[(val_x, val_y), (trn_x, trn_y)],\n              eval_names=['valid', 'train'], early_stopping_rounds=100, verbose=200)\n    \n    oof_preds[val_idx] = model.predict_proba(val_x)[:, 1]\n    sub_preds += model.predict_proba(test)[:, 1] / folds.n_splits\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    del model, trn_x, trn_y, val_x, val_y\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T14:34:15.214909Z","iopub.execute_input":"2022-04-25T14:34:15.216176Z","iopub.status.idle":"2022-04-25T15:09:31.333500Z","shell.execute_reply.started":"2022-04-25T14:34:15.216104Z","shell.execute_reply":"2022-04-25T15:09:31.332536Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"submit['TARGET'] = sub_preds\n\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-25T15:10:02.439683Z","iopub.execute_input":"2022-04-25T15:10:02.441010Z","iopub.status.idle":"2022-04-25T15:10:02.453957Z","shell.execute_reply.started":"2022-04-25T15:10:02.440970Z","shell.execute_reply":"2022-04-25T15:10:02.452950Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('lgb_all_table.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T15:10:08.130862Z","iopub.execute_input":"2022-04-25T15:10:08.131185Z","iopub.status.idle":"2022-04-25T15:10:08.291613Z","shell.execute_reply.started":"2022-04-25T15:10:08.131151Z","shell.execute_reply":"2022-04-25T15:10:08.290730Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Private Score: 0.78106, Public Score: 0.77898.","metadata":{}}]}