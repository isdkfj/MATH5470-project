{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description\n\nThis notebook applies feature selection on merged table.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-01T09:37:01.097778Z","iopub.execute_input":"2022-05-01T09:37:01.098177Z","iopub.status.idle":"2022-05-01T09:37:01.136711Z","shell.execute_reply.started":"2022-05-01T09:37:01.098076Z","shell.execute_reply":"2022-05-01T09:37:01.134766Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../input/\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:00.176236Z","iopub.execute_input":"2022-05-01T09:16:00.176768Z","iopub.status.idle":"2022-05-01T09:16:00.181857Z","shell.execute_reply.started":"2022-05-01T09:16:00.176728Z","shell.execute_reply":"2022-05-01T09:16:00.181288Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def agg_numeric(df, parent_var, df_name):\n    \"\"\"\n    Groups and aggregates the numeric values in a child dataframe\n    by the parent variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the child dataframe to calculate the statistics on\n        parent_var (string): \n            the parent variable used for grouping and aggregating\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated by the `parent_var` for \n            all numeric columns. Each observation of the parent variable will have \n            one row in the dataframe with the parent variable as the index. \n            The columns are also renamed using the `df_name`. Columns with all duplicate\n            values are removed. \n    \n    \"\"\"\n    \n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != parent_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    # Only want the numeric variables\n    parent_ids = df[parent_var].copy()\n    numeric_df = df.select_dtypes('number').copy()\n    numeric_df[parent_var] = parent_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n\n    # Need to create new column names\n    columns = []\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        if var != parent_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n    \n    agg.columns = columns\n    \n    # Remove the columns with all redundant values\n    _, idx = np.unique(agg, axis = 1, return_index=True)\n    agg = agg.iloc[:, idx]\n    \n    return agg","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:00.189069Z","iopub.execute_input":"2022-05-01T09:16:00.189360Z","iopub.status.idle":"2022-05-01T09:16:00.201568Z","shell.execute_reply.started":"2022-05-01T09:16:00.189328Z","shell.execute_reply":"2022-05-01T09:16:00.200220Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def agg_categorical(df, parent_var, df_name):\n    \"\"\"\n    Aggregates the categorical features in a child dataframe\n    for each observation of the parent variable.\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    parent_var : string\n        The variable by which to group and aggregate the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with aggregated statistics for each observation of the parent_var\n        The columns are also renamed and columns with duplicate values are removed.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('category'))\n\n    # Make sure to put the identifying id on the column\n    categorical[parent_var] = df[parent_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['sum', 'count', 'mean']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    # Remove duplicate columns by values\n    _, idx = np.unique(categorical, axis = 1, return_index = True)\n    categorical = categorical.iloc[:, idx]\n    \n    return categorical","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:00.203037Z","iopub.execute_input":"2022-05-01T09:16:00.203718Z","iopub.status.idle":"2022-05-01T09:16:00.222088Z","shell.execute_reply.started":"2022-05-01T09:16:00.203684Z","shell.execute_reply":"2022-05-01T09:16:00.220798Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import sys\n\ndef return_size(df):\n    \"\"\"Return size of dataframe in gigabytes\"\"\"\n    return round(sys.getsizeof(df) / 1e9, 2)\n\ndef convert_types(df, print_info = False):\n    \n    original_memory = df.memory_usage().sum()\n    \n    # Iterate through each column\n    for c in df:\n        \n        # Convert ids and booleans to integers\n        if ('SK_ID' in c):\n            df[c] = df[c].fillna(0).astype(np.int32)\n            \n        # Convert objects to category\n        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n            df[c] = df[c].astype('category')\n        \n        # Booleans mapped to integers\n        elif list(df[c].unique()) == [1, 0]:\n            df[c] = df[c].astype(bool)\n        \n        # Float64 to float32\n        elif df[c].dtype == float:\n            df[c] = df[c].astype(np.float32)\n            \n        # Int64 to int32\n        elif df[c].dtype == int:\n            df[c] = df[c].astype(np.int32)\n        \n    new_memory = df.memory_usage().sum()\n    \n    if print_info:\n        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:37:06.147675Z","iopub.execute_input":"2022-05-01T09:37:06.148010Z","iopub.status.idle":"2022-05-01T09:37:06.159739Z","shell.execute_reply.started":"2022-05-01T09:37:06.147973Z","shell.execute_reply":"2022-05-01T09:37:06.158943Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def aggregate_client(df, group_vars, df_names):\n    \"\"\"Aggregate a dataframe with data at the loan level \n    at the client level\n    \n    Args:\n        df (dataframe): data at the loan level\n        group_vars (list of two strings): grouping variables for the loan \n        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n        names (list of two strings): names to call the resulting columns\n        (example ['cash', 'client'])\n        \n    Returns:\n        df_client (dataframe): aggregated numeric stats at the client level. \n        Each client will have a single row with all the numeric data aggregated\n    \"\"\"\n    \n    # Aggregate the numeric columns\n    df_agg = agg_numeric(df, parent_var=group_vars[0], df_name=df_names[0])\n    \n    # If there are categorical variables\n    if any(df.dtypes == 'category'):\n    \n        # Count the categorical columns\n        df_counts = agg_categorical(df, parent_var=group_vars[0], df_name=df_names[0])\n\n        # Merge the numeric and categorical\n        df_by_loan = df_counts.merge(df_agg, on=group_vars[0], how='outer')\n\n        gc.enable()\n        del df_agg, df_counts\n        gc.collect()\n\n        # Merge to get the client id in dataframe\n        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on=group_vars[0], how='left')\n\n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns=[group_vars[0]])\n\n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var=group_vars[1], df_name=df_names[1])\n\n        \n    # No categorical variables\n    else:\n        # Merge to get the client id in dataframe\n        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on=group_vars[0], how='left')\n        \n        gc.enable()\n        del df_agg\n        gc.collect()\n        \n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns=[group_vars[0]])\n        \n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, parent_var=group_vars[1], df_name=df_names[1])\n        \n    # Memory management\n    gc.enable()\n    del df, df_by_loan\n    gc.collect()\n\n    return df_by_client","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:00.245111Z","iopub.execute_input":"2022-05-01T09:16:00.246005Z","iopub.status.idle":"2022-05-01T09:16:00.266221Z","shell.execute_reply.started":"2022-05-01T09:16:00.245963Z","shell.execute_reply":"2022-05-01T09:16:00.265283Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"bureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\nbureau = convert_types(bureau, print_info=True)\nbureau.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:00.269556Z","iopub.execute_input":"2022-05-01T09:16:00.269905Z","iopub.status.idle":"2022-05-01T09:16:07.983186Z","shell.execute_reply.started":"2022-05-01T09:16:00.269860Z","shell.execute_reply":"2022-05-01T09:16:07.982331Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"bureau_agg = agg_numeric(bureau.drop(columns=['SK_ID_BUREAU']), 'SK_ID_CURR', 'bureau')\nbureau_agg.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:07.984371Z","iopub.execute_input":"2022-05-01T09:16:07.984676Z","iopub.status.idle":"2022-05-01T09:16:14.188128Z","shell.execute_reply.started":"2022-05-01T09:16:07.984643Z","shell.execute_reply":"2022-05-01T09:16:14.187325Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"bureau_counts = agg_categorical(bureau, 'SK_ID_CURR', 'bureau')\nbureau_counts.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:14.189672Z","iopub.execute_input":"2022-05-01T09:16:14.190141Z","iopub.status.idle":"2022-05-01T09:16:22.357887Z","shell.execute_reply.started":"2022-05-01T09:16:14.190099Z","shell.execute_reply":"2022-05-01T09:16:22.357101Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"bureau_balance = pd.read_csv('../input/home-credit-default-risk/bureau_balance.csv')\nbureau_balance = convert_types(bureau_balance, print_info=True)\nbureau_balance.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:22.359337Z","iopub.execute_input":"2022-05-01T09:16:22.359903Z","iopub.status.idle":"2022-05-01T09:16:39.599034Z","shell.execute_reply.started":"2022-05-01T09:16:22.359858Z","shell.execute_reply":"2022-05-01T09:16:39.598386Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"bureau_balance_counts = agg_categorical(bureau_balance, 'SK_ID_BUREAU', 'bureau_balance')\nbureau_balance_counts.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:39.600782Z","iopub.execute_input":"2022-05-01T09:16:39.601106Z","iopub.status.idle":"2022-05-01T09:16:57.335128Z","shell.execute_reply.started":"2022-05-01T09:16:39.601062Z","shell.execute_reply":"2022-05-01T09:16:57.334133Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"bureau_balance_agg = agg_numeric(bureau_balance, 'SK_ID_BUREAU', 'bureau_balance')\nbureau_balance_agg.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:16:57.337802Z","iopub.execute_input":"2022-05-01T09:16:57.338026Z","iopub.status.idle":"2022-05-01T09:17:04.521181Z","shell.execute_reply.started":"2022-05-01T09:16:57.337999Z","shell.execute_reply":"2022-05-01T09:17:04.520218Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Dataframe grouped by the loan\nbureau_by_loan = bureau_balance_agg.merge(bureau_balance_counts, right_index=True, left_on='SK_ID_BUREAU', how='outer')\n\n# Merge to include the SK_ID_CURR\nbureau_by_loan = bureau[['SK_ID_BUREAU', 'SK_ID_CURR']].merge(bureau_by_loan, on='SK_ID_BUREAU', how='left')\n\n# Aggregate the stats for each client\nbureau_balance_by_client = agg_numeric(bureau_by_loan.drop(columns=['SK_ID_BUREAU']), 'SK_ID_CURR', 'client')\n\nbureau_balance_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:04.522577Z","iopub.execute_input":"2022-05-01T09:17:04.522833Z","iopub.status.idle":"2022-05-01T09:17:17.131866Z","shell.execute_reply.started":"2022-05-01T09:17:04.522795Z","shell.execute_reply":"2022-05-01T09:17:17.130865Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Load training data\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\napp_test = convert_types(app_train, print_info=True)\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:17.133471Z","iopub.execute_input":"2022-05-01T09:17:17.133812Z","iopub.status.idle":"2022-05-01T09:17:26.654248Z","shell.execute_reply.started":"2022-05-01T09:17:17.133769Z","shell.execute_reply":"2022-05-01T09:17:26.653110Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Load testing data\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\napp_test = convert_types(app_test, print_info=True)\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:26.655936Z","iopub.execute_input":"2022-05-01T09:17:26.656265Z","iopub.status.idle":"2022-05-01T09:17:28.198619Z","shell.execute_reply.started":"2022-05-01T09:17:26.656202Z","shell.execute_reply":"2022-05-01T09:17:28.197773Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'category':\n        # For binary columns, encode with 0 and 1 (indeed the same as one-hot encoding)\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:28.199886Z","iopub.execute_input":"2022-05-01T09:17:28.200116Z","iopub.status.idle":"2022-05-01T09:17:29.570510Z","shell.execute_reply.started":"2022-05-01T09:17:28.200087Z","shell.execute_reply":"2022-05-01T09:17:29.569550Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\n# The resulting tables (ignore the target column) have different number of columns\n# Because some values occur only in the training data\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:29.571918Z","iopub.execute_input":"2022-05-01T09:17:29.572215Z","iopub.status.idle":"2022-05-01T09:17:30.122759Z","shell.execute_reply.started":"2022-05-01T09:17:29.572176Z","shell.execute_reply":"2022-05-01T09:17:30.121802Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Merge with the value counts of bureau\ntrain = app_train.merge(bureau_counts, on='SK_ID_CURR', how='left')\n\n# Merge with the stats of bureau\ntrain = train.merge(bureau_agg, on='SK_ID_CURR', how='left')\n\n# Merge with the monthly information grouped by client\ntrain = train.merge(bureau_balance_by_client, on='SK_ID_CURR', how='left')\n\nprint('Training data shape: ', train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:30.124097Z","iopub.execute_input":"2022-05-01T09:17:30.124371Z","iopub.status.idle":"2022-05-01T09:17:34.338254Z","shell.execute_reply.started":"2022-05-01T09:17:30.124340Z","shell.execute_reply":"2022-05-01T09:17:34.337260Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Merge with the value counts of bureau\ntest = app_test.merge(bureau_counts, on='SK_ID_CURR', how='left')\n\n# Merge with the stats of bureau\ntest = test.merge(bureau_agg, on='SK_ID_CURR', how='left')\n\n# Merge with the value counts of bureau balance\ntest = test.merge(bureau_balance_by_client, on='SK_ID_CURR', how='left')\n\nprint('Testing data shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:34.340377Z","iopub.execute_input":"2022-05-01T09:17:34.341302Z","iopub.status.idle":"2022-05-01T09:17:35.334938Z","shell.execute_reply.started":"2022-05-01T09:17:34.341233Z","shell.execute_reply":"2022-05-01T09:17:35.334037Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"gc.enable()\ndel bureau, bureau_counts, bureau_agg, bureau_balance, bureau_by_loan, bureau_balance_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:35.336384Z","iopub.execute_input":"2022-05-01T09:17:35.336625Z","iopub.status.idle":"2022-05-01T09:17:35.462972Z","shell.execute_reply.started":"2022-05-01T09:17:35.336596Z","shell.execute_reply":"2022-05-01T09:17:35.462340Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"previous = pd.read_csv('../input/home-credit-default-risk/previous_application.csv')\nprevious = convert_types(previous, print_info=True)\nprevious.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:35.464006Z","iopub.execute_input":"2022-05-01T09:17:35.464531Z","iopub.status.idle":"2022-05-01T09:17:58.871538Z","shell.execute_reply.started":"2022-05-01T09:17:35.464495Z","shell.execute_reply":"2022-05-01T09:17:58.870607Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Calculate aggregate statistics for each numeric column\nprevious_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\nprint('Previous aggregation shape: ', previous_agg.shape)\nprevious_agg.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:17:58.872880Z","iopub.execute_input":"2022-05-01T09:17:58.873384Z","iopub.status.idle":"2022-05-01T09:18:08.984463Z","shell.execute_reply.started":"2022-05-01T09:17:58.873339Z","shell.execute_reply":"2022-05-01T09:18:08.983548Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Calculate value counts for each categorical column\nprevious_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\nprint('Previous counts shape: ', previous_counts.shape)\nprevious_counts.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:18:08.985875Z","iopub.execute_input":"2022-05-01T09:18:08.986096Z","iopub.status.idle":"2022-05-01T09:18:58.890050Z","shell.execute_reply.started":"2022-05-01T09:18:08.986069Z","shell.execute_reply":"2022-05-01T09:18:58.889009Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Merge in the previous information\ntrain = train.merge(previous_counts, on='SK_ID_CURR', how='left')\ntrain = train.merge(previous_agg, on='SK_ID_CURR', how='left')\n\ntest = test.merge(previous_counts, on='SK_ID_CURR', how='left')\ntest = test.merge(previous_agg, on='SK_ID_CURR', how='left')\n\n# Remove variables to free memory\ngc.enable()\ndel previous, previous_agg, previous_counts\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:18:58.891425Z","iopub.execute_input":"2022-05-01T09:18:58.891741Z","iopub.status.idle":"2022-05-01T09:19:42.087977Z","shell.execute_reply.started":"2022-05-01T09:18:58.891689Z","shell.execute_reply":"2022-05-01T09:19:42.087002Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"cash = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv')\ncash = convert_types(cash, print_info=True)\ncash.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:19:42.089371Z","iopub.execute_input":"2022-05-01T09:19:42.089599Z","iopub.status.idle":"2022-05-01T09:19:55.080481Z","shell.execute_reply.started":"2022-05-01T09:19:42.089572Z","shell.execute_reply":"2022-05-01T09:19:55.079521Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"cash_by_client = aggregate_client(cash, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['cash', 'client'])\ncash_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:19:55.084502Z","iopub.execute_input":"2022-05-01T09:19:55.084751Z","iopub.status.idle":"2022-05-01T09:21:33.097132Z","shell.execute_reply.started":"2022-05-01T09:19:55.084724Z","shell.execute_reply":"2022-05-01T09:21:33.096503Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print('Cash by Client Shape: ', cash_by_client.shape)\n\ntrain = train.merge(cash_by_client, on='SK_ID_CURR', how='left')\ntest = test.merge(cash_by_client, on='SK_ID_CURR', how='left')\n\ngc.enable()\ndel cash, cash_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:21:33.098628Z","iopub.execute_input":"2022-05-01T09:21:33.099032Z","iopub.status.idle":"2022-05-01T09:21:40.804372Z","shell.execute_reply.started":"2022-05-01T09:21:33.098984Z","shell.execute_reply":"2022-05-01T09:21:40.803701Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"credit = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv')\ncredit = convert_types(credit, print_info=True)\ncredit.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:21:40.805538Z","iopub.execute_input":"2022-05-01T09:21:40.806186Z","iopub.status.idle":"2022-05-01T09:22:02.328720Z","shell.execute_reply.started":"2022-05-01T09:21:40.806153Z","shell.execute_reply":"2022-05-01T09:22:02.327684Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"credit_by_client = aggregate_client(credit, group_vars=['SK_ID_PREV', 'SK_ID_CURR'], df_names=['credit', 'client'])\ncredit_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:22:02.329867Z","iopub.execute_input":"2022-05-01T09:22:02.330078Z","iopub.status.idle":"2022-05-01T09:22:45.361857Z","shell.execute_reply.started":"2022-05-01T09:22:02.330053Z","shell.execute_reply":"2022-05-01T09:22:45.360922Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print('Credit by client shape: ', credit_by_client.shape)\n\ntrain = train.merge(credit_by_client, on='SK_ID_CURR', how='left')\ntest = test.merge(credit_by_client, on='SK_ID_CURR', how='left')\n\ngc.enable()\ndel credit, credit_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:22:45.363300Z","iopub.execute_input":"2022-05-01T09:22:45.363610Z","iopub.status.idle":"2022-05-01T09:23:03.134672Z","shell.execute_reply.started":"2022-05-01T09:22:45.363571Z","shell.execute_reply":"2022-05-01T09:23:03.133561Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"installments = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv')\ninstallments = convert_types(installments, print_info=True)\ninstallments.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:23:03.136166Z","iopub.execute_input":"2022-05-01T09:23:03.136452Z","iopub.status.idle":"2022-05-01T09:23:28.438703Z","shell.execute_reply.started":"2022-05-01T09:23:03.136421Z","shell.execute_reply":"2022-05-01T09:23:28.437572Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\ninstallments_by_client.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:23:28.440577Z","iopub.execute_input":"2022-05-01T09:23:28.440946Z","iopub.status.idle":"2022-05-01T09:24:37.043747Z","shell.execute_reply.started":"2022-05-01T09:23:28.440901Z","shell.execute_reply":"2022-05-01T09:24:37.042708Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print('Installments by client shape: ', installments_by_client.shape)\n\ntrain = train.merge(installments_by_client, on='SK_ID_CURR', how='left')\ntest = test.merge(installments_by_client, on='SK_ID_CURR', how='left')\n\ngc.enable()\ndel installments, installments_by_client\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:24:37.045071Z","iopub.execute_input":"2022-05-01T09:24:37.045325Z","iopub.status.idle":"2022-05-01T09:24:40.460696Z","shell.execute_reply.started":"2022-05-01T09:24:37.045295Z","shell.execute_reply":"2022-05-01T09:24:40.459844Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def remove_missing_columns(train, test, threshold = 70):\n    # Calculate missing stats for train and test (remember to calculate a percent!)\n    train_miss = pd.DataFrame(train.isnull().sum())\n    train_miss['percent'] = 100 * train_miss[0] / len(train)\n    \n    test_miss = pd.DataFrame(test.isnull().sum())\n    test_miss['percent'] = 100 * test_miss[0] / len(test)\n    \n    # list of missing columns for train and test\n    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n    \n    # Combine the two lists together\n    missing_columns = list(set(missing_train_columns + missing_test_columns))\n    \n    # Print information\n    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n    \n    # Drop the missing columns and return\n    train = train.drop(columns = missing_columns)\n    test = test.drop(columns = missing_columns)\n    \n    return train, test","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:24:40.461899Z","iopub.execute_input":"2022-05-01T09:24:40.462200Z","iopub.status.idle":"2022-05-01T09:24:40.471019Z","shell.execute_reply.started":"2022-05-01T09:24:40.462158Z","shell.execute_reply":"2022-05-01T09:24:40.470288Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train, test = remove_missing_columns(train, test)\nprint('Final Training Shape: ', train.shape)\nprint('Final Testing Shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:24:40.472319Z","iopub.execute_input":"2022-05-01T09:24:40.472558Z","iopub.status.idle":"2022-05-01T09:24:44.939302Z","shell.execute_reply.started":"2022-05-01T09:24:40.472525Z","shell.execute_reply":"2022-05-01T09:24:44.938172Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_labels = train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\ntrain['TARGET'] = train_labels\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:24:44.940741Z","iopub.execute_input":"2022-05-01T09:24:44.941046Z","iopub.status.idle":"2022-05-01T09:24:45.538873Z","shell.execute_reply.started":"2022-05-01T09:24:44.941016Z","shell.execute_reply":"2022-05-01T09:24:45.537945Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Save the merged train and test dataset\ntrain.to_csv('train.csv', index=False)\ntest.to_csv('test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:24:45.539987Z","iopub.execute_input":"2022-05-01T09:24:45.540206Z","iopub.status.idle":"2022-05-01T09:29:14.279495Z","shell.execute_reply.started":"2022-05-01T09:24:45.540179Z","shell.execute_reply":"2022-05-01T09:29:14.278491Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Directly load saved dataframe\ntrain = pd.read_csv('../input/home-credit-merged/train.csv')\ntest = pd.read_csv('../input/home-credit-merged/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:37:22.007541Z","iopub.execute_input":"2022-05-01T09:37:22.007847Z","iopub.status.idle":"2022-05-01T09:38:36.302348Z","shell.execute_reply.started":"2022-05-01T09:37:22.007817Z","shell.execute_reply":"2022-05-01T09:38:36.301359Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = convert_types(train, print_info=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:38:36.304102Z","iopub.execute_input":"2022-05-01T09:38:36.304396Z","iopub.status.idle":"2022-05-01T09:43:12.247014Z","shell.execute_reply.started":"2022-05-01T09:38:36.304360Z","shell.execute_reply":"2022-05-01T09:43:12.243113Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test = convert_types(test, print_info=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:43:12.249736Z","iopub.execute_input":"2022-05-01T09:43:12.250116Z","iopub.status.idle":"2022-05-01T09:43:51.263883Z","shell.execute_reply.started":"2022-05-01T09:43:12.250065Z","shell.execute_reply":"2022-05-01T09:43:51.262786Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = train.sample(n=1000, random_state=233).corr().abs()\ncorr_matrix.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:43:51.265927Z","iopub.execute_input":"2022-05-01T09:43:51.266200Z","iopub.status.idle":"2022-05-01T09:43:54.803567Z","shell.execute_reply.started":"2022-05-01T09:43:51.266166Z","shell.execute_reply":"2022-05-01T09:43:54.802075Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\nprint(len(to_drop), 'columns need to be deleted.')","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:43:54.805509Z","iopub.execute_input":"2022-05-01T09:43:54.806287Z","iopub.status.idle":"2022-05-01T09:43:55.106202Z","shell.execute_reply.started":"2022-05-01T09:43:54.806154Z","shell.execute_reply":"2022-05-01T09:43:55.105372Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train = train.drop(train[to_drop], axis=1)\ntest = test.drop(test[to_drop], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:43:55.107491Z","iopub.execute_input":"2022-05-01T09:43:55.108551Z","iopub.status.idle":"2022-05-01T09:43:55.662086Z","shell.execute_reply.started":"2022-05-01T09:43:55.108498Z","shell.execute_reply":"2022-05-01T09:43:55.660831Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:43:55.664513Z","iopub.execute_input":"2022-05-01T09:43:55.664781Z","iopub.status.idle":"2022-05-01T09:43:55.672122Z","shell.execute_reply.started":"2022-05-01T09:43:55.664751Z","shell.execute_reply":"2022-05-01T09:43:55.670748Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\n\ntrain_labels = train['TARGET']\ntrain_id = train[['SK_ID_CURR']]\ntrain = train.drop(columns=['SK_ID_CURR'])\ntest_id = test[['SK_ID_CURR']]\ntest = test.drop(columns=['SK_ID_CURR'])\ntrain = train.drop(columns = ['TARGET'])\n\nimport re\ntrain = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ntest = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\n# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:43:55.673856Z","iopub.execute_input":"2022-05-01T09:43:55.674270Z","iopub.status.idle":"2022-05-01T09:43:59.450029Z","shell.execute_reply.started":"2022-05-01T09:43:55.674180Z","shell.execute_reply":"2022-05-01T09:43:59.448821Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# K-fold cross validation\nfrom sklearn.model_selection import KFold\nfolds = KFold(n_splits=10, shuffle=True, random_state=233)\n\nfrom sklearn.metrics import roc_auc_score\n\noof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train)):\n    trn_x, trn_y = train.iloc[trn_idx], train_labels.iloc[trn_idx]\n    val_x, val_y = train.iloc[val_idx], train_labels.iloc[val_idx]\n    \n    # Create Lightgbm model\n    model = lgb.LGBMClassifier(n_estimators=10000, objective='binary', \n                               class_weight='balanced', learning_rate=0.05, \n                               reg_alpha=0.1, reg_lambda=0.1, \n                               subsample=0.8, n_jobs=-1, random_state=233)\n    \n    model.fit(trn_x, trn_y, eval_metric='auc', eval_set=[(val_x, val_y), (trn_x, trn_y)],\n              eval_names=['valid', 'train'], early_stopping_rounds=100, verbose=200)\n    \n    oof_preds[val_idx] = model.predict_proba(val_x)[:, 1]\n    sub_preds += model.predict_proba(test)[:, 1] / folds.n_splits\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    feature_importances += model.feature_importances_\n    del model, trn_x, trn_y, val_x, val_y\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T09:43:59.451795Z","iopub.execute_input":"2022-05-01T09:43:59.452046Z","iopub.status.idle":"2022-05-01T10:17:08.763698Z","shell.execute_reply.started":"2022-05-01T09:43:59.452017Z","shell.execute_reply":"2022-05-01T10:17:08.762694Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"feature_importances = feature_importances / 10\nfeature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n\nfeature_importances.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:17:08.766608Z","iopub.execute_input":"2022-05-01T10:17:08.767460Z","iopub.status.idle":"2022-05-01T10:17:08.783474Z","shell.execute_reply.started":"2022-05-01T10:17:08.767391Z","shell.execute_reply":"2022-05-01T10:17:08.782180Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:17:08.785522Z","iopub.execute_input":"2022-05-01T10:17:08.785978Z","iopub.status.idle":"2022-05-01T10:17:08.812837Z","shell.execute_reply.started":"2022-05-01T10:17:08.785927Z","shell.execute_reply":"2022-05-01T10:17:08.811929Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 22\nimport seaborn as sns\n\ndef plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 15 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for prining information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 18\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:5]))), \n            df['importance_normalized'].head(5), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:5]))))\n    ax.set_yticklabels(df['feature'].head(5))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:17:08.814674Z","iopub.execute_input":"2022-05-01T10:17:08.815669Z","iopub.status.idle":"2022-05-01T10:17:09.060050Z","shell.execute_reply.started":"2022-05-01T10:17:08.815619Z","shell.execute_reply":"2022-05-01T10:17:09.059036Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"norm_feature_importances = plot_feature_importances(feature_importances)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:17:09.061390Z","iopub.execute_input":"2022-05-01T10:17:09.061632Z","iopub.status.idle":"2022-05-01T10:17:09.467515Z","shell.execute_reply.started":"2022-05-01T10:17:09.061602Z","shell.execute_reply":"2022-05-01T10:17:09.466442Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train = train.drop(columns = zero_features)\ntest = test.drop(columns = zero_features)\ntrain['SK_ID_CURR'] = train_id['SK_ID_CURR']\ntest['SK_ID_CURR'] = test_id['SK_ID_CURR']\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:17:09.469439Z","iopub.execute_input":"2022-05-01T10:17:09.470179Z","iopub.status.idle":"2022-05-01T10:17:09.773577Z","shell.execute_reply.started":"2022-05-01T10:17:09.470127Z","shell.execute_reply":"2022-05-01T10:17:09.772737Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train['TARGET'] = train_labels\n\n# Save the selected train and test dataset\ntrain.to_csv('train.csv', index=False)\ntest.to_csv('test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:17:09.774866Z","iopub.execute_input":"2022-05-01T10:17:09.775640Z","iopub.status.idle":"2022-05-01T10:19:50.700493Z","shell.execute_reply.started":"2022-05-01T10:17:09.775599Z","shell.execute_reply":"2022-05-01T10:19:50.699366Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}